{"cells":[{"cell_type":"markdown","metadata":{"id":"RBVmnjjE-Rtp"},"source":["## Setup and Data Prep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pT-zaDUh-Rtp"},"outputs":[],"source":["%pip install -q diffusers  # Installing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2yyO5qH-Rtr"},"outputs":[],"source":["import torch\n","import torchvision\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader\n","from diffusers import DDPMScheduler, UNet2DModel\n","from matplotlib import pyplot as plt\n","from tqdm.auto import tqdm\n","# Identify and choose device\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using device: {device}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OeMklCBz-Rtr"},"outputs":[],"source":["#Loading the Fahion minst dataset a dataset containing images and numerical class labels\n","dataset = torchvision.datasets.FashionMNIST(root=\"FashionMNIST/\", train=True, download=True, transform=torchvision.transforms.ToTensor())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v1nibTd9x4oY"},"outputs":[],"source":["# Stealing a predefined dataloader to show examples\n","train_dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n","\n","# Showing examples from dataset\n","x, y = next(iter(train_dataloader))\n","print('Input shape:', x.shape)\n","print('Labels:', y)\n","plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys');"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aQlPCTK-Rtr"},"outputs":[],"source":["class class_conditioned_unet(nn.Module):\n","  def __init__(self, num_classes=10, class_emb_size=100):\n","    super().__init__()\n","\n","    # Embedding layer will map the class label to a vector of size class_emb_size\n","    self.class_emb = nn.Embedding(num_classes, class_emb_size)\n","\n","    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n","    self.model = UNet2DModel(\n","        sample_size=28,           # the target image resolution\n","        in_channels=1 + class_emb_size, # Additional input for class condition\n","        out_channels=1,           # grea\n","        layers_per_block=2,       # how many ResNet layers to use per UNet block\n","        block_out_channels=(32, 64, 64),\n","        down_block_types=(\n","            \"DownBlock2D\",        # Regular ResNet downsampling block\n","            \"AttnDownBlock2D\",    # Downsampling block with spatial self-attention\n","            \"AttnDownBlock2D\",\n","        ),\n","        up_block_types=(\n","            \"AttnUpBlock2D\",\n","            \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n","            \"UpBlock2D\",          # a regular ResNet upsampling block\n","          ),\n","    )\n","\n","  # forward method  takes the class labels as an additional argument\n","  def forward(self, x, t, class_labels):\n","    # Shape of x:\n","    bs, ch, w, h = x.shape\n","\n","    # class conditioning in right shape to give as additional input\n","    class_cond = self.class_emb(class_labels) # Map to embedding dimension\n","    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h) # Expand to contain class embedding\n","\n","    # Net input is now x and class cond concatenated together along dimension 1\n","    net_input = torch.cat((x, class_cond), 1) # concat to one dim input\n","\n","    # Feed this to the UNet alongside the timestep and return the prediction\n","    return self.model(net_input, t).sample # (bs, 1, 28, 28)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u7LJndHq-Rts"},"outputs":[],"source":["# Create noise\n","noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cOcosBrq-Rts"},"outputs":[],"source":["# Train dataloader\n","train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n","\n","# Number of epochs\n","n_epochs = 15\n","\n","# Our network\n","net = class_conditioned_unet().to(device)\n","\n","# Our loss function\n","loss_fn = nn.MSELoss()\n","\n","# Adam  optimizer and learning rate of\n","opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n","\n","# Save loss for plot later\n","losses = []\n","\n","# Thraditional training loop\n","for epoch in range(n_epochs):\n","    for x, y in tqdm(train_dataloader):\n","\n","        # Retrive data and corrupt it with noise\n","        x = x.to(device) * 2 - 1\n","        y = y.to(device)\n","        noise = torch.randn_like(x) # Generate noise\n","        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(device) # Determine the dregree in which the image is gradually turned into noise,\n","        noisy_x = noise_scheduler.add_noise(x, noise, timesteps) # Add noise  for the given timestep\n","\n","        # Get the model prediction\n","        pred = net(noisy_x, timesteps, y)\n","\n","        # Calculate the loss\n","        loss = loss_fn(pred, noise)\n","\n","        # Backprop and update the params:\n","        opt.zero_grad()\n","        loss.backward()\n","        opt.step()\n","\n","        # Store loss\n","        losses.append(loss.item())\n","\n","    # Average of the last 50\n","    avg_loss = sum(losses[-50:])/50\n","    print(f'Finished epoch {epoch}. Average of the last 50 loss values: {avg_loss:05f}')\n","\n","# View the loss curve\n","plt.plot(losses)"]},{"cell_type":"markdown","metadata":{"id":"ji_kIVBe-Rtt"},"source":["Once training finishes, we can sample some images feeding in different labels as our conditioning:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZeC4_njJ-Rtt"},"outputs":[],"source":["def prediction(a, b):\n","  a = a # number of images/predictions for each number given as input\n","  b = b # range of number u want to predict remember we start in zero\n","  z = a * b\n","  x = torch.rand(z, 1, 28, 28).to(device)\n","  y = torch.tensor([[i]*a for i in range(b)]).flatten().to(device)\n","\n","  # Sampling loop\n","  for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n","\n","    # Do model prediction\n","      with torch.no_grad():\n","          residual = net(x, t, y)\n","\n","    # Update sample with step\n","      x = noise_scheduler.step(residual, t, x).prev_sample\n","\n","  # Results\n","  fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n","  ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-1, 1), nrow=b)[0], cmap='Greys')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BATanoEOeacs"},"outputs":[],"source":["text = int(input(\"Number of predictions for each category: \"))\n","cnt = int(input(\"Numbers correcspoding to clothes categories you want to predic if e.g. 2 (0, 1) will be predicted: \"))\n","prediction(text, cnt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3QGAPaoJ-Rtt"},"outputs":[],"source":["# Exercise (optional): Try this with FashionMNIST. Tweak the learning rate, batch size and number of epochs.\n","# Can you get some decent-looking fashion images with less training time than the example above?"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1OPJwJrl_gir562VN0YF8ISamZV4jA0x8","timestamp":1701250059397},{"file_id":"1Ot7n_trFqFfKWG4HVKKWBdaCvEFy0dpK","timestamp":1700061866235},{"file_id":"https://github.com/huggingface/diffusion-models-class/blob/main/unit2/02_class_conditioned_diffusion_model_example.ipynb","timestamp":1700052725123}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}