{"cells":[{"cell_type":"markdown","source":["# Simplified Diffusion Model\n","**Remember to enable GPU**\n","\n","This notebook demonstrates how to implement and train a simple diffusion model to generate images of aquariun fish. The model is kept simple for teaching purposes.\n","\n","For an implementation that is closer to the original, you are referred to [this tutorial](https://keras.io/examples/generative/ddpm/).\n","\n","## Your task\n","Your task is simply to see if you can match the code against the theory presented in the slides of Lecture 13. Focus on understanding\n","- the noise scheduler\n","- the forward diffusion process\n","- the U-net architecture, in particular the temporal embeddings\n","- the reverse diffusion process (also called sampling)"],"metadata":{"id":"HyVEolyKowux"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"12R2lBUFyAja"},"outputs":[],"source":["import numpy as np\n","from tqdm.auto import trange, tqdm\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","source":["## 1. Download data\n","We will be using the \"aquariun fish\" images of the CIFAR101 dataset."],"metadata":{"id":"fnGhVshNybGv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCIod8hgyKcT"},"outputs":[],"source":["(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data(label_mode='fine')\n","X_train = X_train[y_train.squeeze() == 1] # restrict class label to \"aquarium_fish\" (full class list here: https://github.com/MartinThoma/algorithms/blob/master/ML/confusion-matrix/labels/cifar-100-labels.json)\n","X_train = (X_train / 127.5) - 1.0"]},{"cell_type":"code","source":["IMG_SIZE = 32     # img_size CIFAR-10 img_size is 32x32 and therefore input img size is 32\n","BATCH_SIZE = 128  # batch size"],"metadata":{"id":"e4IcR2ckzl3q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2. Utility functions"],"metadata":{"id":"6HKl80IHzv4o"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1rZ-Wy1yRqk"},"outputs":[],"source":["from pickle import NONE\n","\n","def cvtImg(img):\n","    # Normalize image intensities\n","    img = img - img.min()\n","    img = (img / img.max())\n","    return img.astype(np.float32)\n","\n","def show_examples(x,t=None):\n","    # Show multiple images\n","    plt.figure(figsize=(10, 10))\n","    for i in range(16):\n","        plt.subplot(4, 4, i+1)\n","        img = cvtImg(x[i])\n","        plt.imshow(img)\n","        if t is not None:\n","          plt.title(f\"t={t[i]}\")\n","        plt.axis('off')"]},{"cell_type":"markdown","source":["## 3. Noise schedule\n","Recall that in the forward diffusion process, the noise increases gradually over time according to some schedule. Here, we are using a linear schedule of 16 timesteps."],"metadata":{"id":"tmzrlbDoyqrB"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dT4fx3dyNrJ"},"outputs":[],"source":["n_timesteps = 16    # number of time steps in the diffusion process (corresponds to T in the slides of lecture 13)\n","betas = np.linspace(0, 1.0, n_timesteps + 1) # noise variance schedule for adding noise to the image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"09U_85ctyPMo"},"outputs":[],"source":["plt.plot(betas, label='Noise schedule (beta)')\n","plt.legend()"]},{"cell_type":"markdown","source":["## 4. Forward diffusion\n","Due to the way the training loop works, we will define a function that generates two consecutive images for time t and time t+1. Both images are based on the same base image (x_0) and the same random noise:\n","- The image at time t has more weight on x_0 and a lower loise level (according to the noise schedule).\n","- The image at time t+1 has less weight on x_0 and a higher loise level\n"],"metadata":{"id":"HFxVQJ5y01u5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qtk8tTguyUbE"},"outputs":[],"source":["def forward_noise(x_0, t):\n","    # Function used for forward diffusion\n","    a = betas[t]      # noise level for t\n","    b = betas[t + 1]  # noise level for t + 1 (higher)\n","\n","    noise = np.random.normal(size=x_0.shape)  # random noise\n","\n","    # Generate the two images\n","    a = a.reshape((-1, 1, 1, 1))\n","    b = b.reshape((-1, 1, 1, 1))\n","    img_a = x_0 * (1 - a) + noise * a # image at time t\n","    img_b = x_0 * (1 - b) + noise * b # image at time t+1\n","\n","    return img_a, img_b"]},{"cell_type":"markdown","source":["### 3.1 Example of forward diffusion process\n","Here, we are using the same base image to generate images at all time steps of the forward diffusion process. Note that the noise level are determined by t according to the noise schedule (betas)."],"metadata":{"id":"1A2wke-k8VcR"}},{"cell_type":"code","source":["t = np.array(list(range(n_timesteps))) # all time steps\n","image_index = 2 # selected base image\n","indices = [image_index for i in range(n_timesteps)] # hack to select the same image for each time step\n","a, b = forward_noise(X_train[indices], t)\n","show_examples(a,t)"],"metadata":{"id":"7Ybr0UAs4mGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3.1 Generating a training batch\n","Let's consider the first 16 training images."],"metadata":{"id":"HwMsH_69B-VL"}},{"cell_type":"code","source":["show_examples(X_train[:16])"],"metadata":{"id":"k9Q7fOQE3vGy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, let is generate some training pairs for these 16 images. For each image, we will select `t` at random."],"metadata":{"id":"BMCynOv4CX-B"}},{"cell_type":"code","source":["def generate_ts(num):\n","    return np.random.randint(0, n_timesteps, size=num)\n","\n","# Random choices of t\n","t = generate_ts(16)\n","\n","# Training pairs\n","a, b = forward_noise(X_train[:16], t)"],"metadata":{"id":"vl5xgeXh8H3O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Show images at time t (ledd noise):"],"metadata":{"id":"bYFO7nj_CsvL"}},{"cell_type":"code","source":["show_examples(a,t)"],"metadata":{"id":"ofIjkXxqCwiX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Show images at time t+1 (more noise):"],"metadata":{"id":"UpzmzEt-C1dH"}},{"cell_type":"code","source":["show_examples(b,t)"],"metadata":{"id":"YKQKOMv3C5gJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. U-net architecture\n","We will now implement the U-net architecture. Recall that this is a special variant of the U-net that also takes time `t` as input. This is done using learnable time embeddings, that are added to each layer of the U-net."],"metadata":{"id":"hjdNMhesDUmo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJyQGw1ZyXUK"},"outputs":[],"source":["# Basic U-net block\n","def block(x_img, x_ts, n_filters=128):\n","    # Convolution\n","    img_embedding = layers.Conv2D(n_filters, kernel_size=3, padding='same')(x_img)\n","    img_embedding = layers.Activation('relu')(img_embedding)\n","\n","    # Time embedding\n","    time_embedding = layers.Dense(n_filters)(x_ts)\n","    time_embedding = layers.Activation('relu')(time_embedding)\n","    time_embedding = layers.Reshape((1, 1, n_filters))(time_embedding)\n","\n","    # Combine embeddings\n","    combi_embedding = img_embedding * time_embedding\n","\n","    # Final embedding\n","    embedding = layers.Conv2D(n_filters, kernel_size=3, padding='same')(x_img)\n","    embedding = embedding + combi_embedding\n","    embedding = layers.LayerNormalization()(embedding)\n","    embedding = layers.Activation('relu')(embedding)\n","\n","    return embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bc5BckA2ybcj"},"outputs":[],"source":["def build_unet():\n","\n","    # Image\n","    x = x_input = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3), name='x_input')\n","\n","    # Time step (t)\n","    x_ts = x_ts_input = layers.Input(shape=(1,), name='x_ts_input')\n","    x_ts = layers.Dense(192)(x_ts)\n","    x_ts = layers.LayerNormalization()(x_ts)\n","    x_ts = layers.Activation('relu')(x_ts)\n","\n","    # ----- left ( downsampling part of U-net ) -----\n","    x = x32 = block(x, x_ts)\n","    x = layers.MaxPool2D(2)(x)\n","\n","    x = x16 = block(x, x_ts)\n","    x = layers.MaxPool2D(2)(x)\n","\n","    x = x8 = block(x, x_ts)\n","    x = layers.MaxPool2D(2)(x)\n","\n","    x = x4 = block(x, x_ts)\n","\n","    # ----- MLP (U-net bottleneck) -----\n","    x = layers.Flatten()(x)\n","    x = layers.Concatenate()([x, x_ts])\n","    x = layers.Dense(128)(x)\n","    x = layers.LayerNormalization()(x)\n","    x = layers.Activation('relu')(x)\n","\n","    x = layers.Dense(4 * 4 * 32)(x)\n","    x = layers.LayerNormalization()(x)\n","    x = layers.Activation('relu')(x)\n","    x = layers.Reshape((4, 4, 32))(x)\n","\n","    # ----- right ( upsampling part of U-net ) -----\n","    x = layers.Concatenate()([x, x4])\n","    x = block(x, x_ts)\n","    x = layers.UpSampling2D(2)(x)\n","\n","    x = layers.Concatenate()([x, x8])\n","    x = block(x, x_ts)\n","    x = layers.UpSampling2D(2)(x)\n","\n","    x = layers.Concatenate()([x, x16])\n","    x = block(x, x_ts)\n","    x = layers.UpSampling2D(2)(x)\n","\n","    x = layers.Concatenate()([x, x32])\n","    x = block(x, x_ts)\n","\n","    # ----- output -----\n","    x = layers.Conv2D(3, kernel_size=1, padding='same')(x)\n","    model = tf.keras.models.Model([x_input, x_ts_input], x)\n","    return model\n","\n","model = build_unet()\n","model.summary()"]},{"cell_type":"markdown","source":["### 4.1 Test by running inference using the *untrained* U-net\n","To generate an image, we need to run the *reverse diffusion* process. This process starts with an image that is just random noise, and then iteratively denoises the image for time going from T to 0."],"metadata":{"id":"8vDvgIQdGHYG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cj3EpYxbygNe"},"outputs":[],"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=0.0008)\n","loss_func = tf.keras.losses.MeanAbsoluteError()\n","model.compile(loss=loss_func, optimizer=optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oG9wJ_wJyjPL"},"outputs":[],"source":["# This illustrates the reverse diffusion process for a single image\n","# (every other time step)\n","def reverse_diffusion_steps(x_idx=None): # untrianed model output\n","    xs = [] # store all images from reverse diffusion here\n","\n","    # input is just random noise\n","    x = np.random.normal(size=(1, IMG_SIZE, IMG_SIZE, 3))\n","\n","    for t in reversed(range(n_timesteps)):\n","        x = model.predict([x, np.full((1), t)], verbose=0)\n","        if t % 2 == 0:\n","            xs.append(x[0])\n","\n","    plt.figure(figsize=(20, 2))\n","    for i in range(len(xs)):\n","        plt.subplot(1, len(xs), i+1)\n","        plt.imshow(cvtImg(xs[i]))\n","        plt.title(f'{i}')\n","        plt.axis('off')\n","\n","reverse_diffusion_steps()"]},{"cell_type":"code","source":["# This generates a batch of 16 images, each with a different starting point (random noise)\n","def reverse_diffusion_batch(x_idx=None): # untrianed model output\n","    all_x = [] # store all images from reverse diffusion here\n","\n","    # input is just random noise\n","    x = np.random.normal(size=(16, IMG_SIZE, IMG_SIZE, 3))\n","\n","    for t in reversed(range(n_timesteps)):\n","        x = model.predict([x, np.full((16), t)], verbose=0)\n","\n","    show_examples(x)\n","\n","reverse_diffusion_batch()"],"metadata":{"id":"O7ANswherL4c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5. Model training\n","Don't expect impressive results with this simple implementation.\n","\n","But notice how the images become more and more fish-like, as the model trains :-)"],"metadata":{"id":"xAZdRVEbtnlD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"30Wk90LoyybA"},"outputs":[],"source":["def train_batch(x_img):\n","    # Train on a single batch\n","    #   x_a is the image at time t\n","    #   x_b is the image at time t+1\n","    # The model is trained to predict x_a from x_b (reverse diffusion)\n","\n","    x_ts = generate_ts(len(x_img)) # assign random time t to each image in the batch\n","    x_a, x_b = forward_noise(x_img, x_ts) # forward diffusion\n","    loss = model.train_on_batch([x_b, x_ts], x_a) # predict x_a from x_b (and time t)\n","    return loss\n","\n","def train_epoch(R=50):\n","    # Train on multiple batches (specified by R)\n","    bar = trange(R)\n","    total = 10\n","    for i in bar:\n","        for j in range(total):\n","            x_img = X_train[np.random.randint(len(X_train), size=BATCH_SIZE)]\n","            loss = train_batch(x_img)\n","            pg = (j / total) * 100\n","        bar.set_description(f'loss: {loss:.5f}, p: {pg:.2f}%')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sql6X_yIy4jr"},"outputs":[],"source":["for i in range(100):\n","    train_epoch()\n","    # reduce learning rate for next training\n","    model.optimizer.learning_rate = max(0.000001, model.optimizer.learning_rate * 0.99)\n","\n","    # show test result\n","    reverse_diffusion_steps()\n","    reverse_diffusion_batch()\n","    plt.show()"]},{"cell_type":"code","source":[],"metadata":{"id":"gHLd0O-h5KtS"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1NbQ8aBG--JswZBJLoWCMRbDxQcatjFzJ","timestamp":1701250085548},{"file_id":"1UCpLmaZp_vi_IGzWUL0gPAPYqxc96_yt","timestamp":1701177065028}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}